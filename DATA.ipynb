{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWBKY17kFueS"
   },
   "source": [
    "Vamos a comenzarr con la creación de la base de datos y el Análisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZiRZ0u-qrMt"
   },
   "source": [
    "Lo primero que haremos será la automatización de la descarga de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBa5gdqczCjm"
   },
   "source": [
    "primero los de calidad del aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OObbe4NqwDf",
    "outputId": "0283d6c2-0d7e-46d1-c1f0-971db1700d3b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "input_folder_aire = \"/content/sample_data/Calidad_del_aire_original\"\n",
    "input_folder_meteoro='/content/sample_data/Meteorología_Original'\n",
    "\n",
    "os.makedirs(input_folder_aire, exist_ok=True)\n",
    "os.makedirs(input_folder_meteoro, exist_ok=True)\n",
    "\n",
    "urls = {\n",
    "    2024: \"https://datos.madrid.es/egob/catalogo/201200-10306320-calidad-aire-horario.zip\",\n",
    "    2023: \"https://datos.madrid.es/egob/catalogo/201200-10306319-calidad-aire-horario.zip\",\n",
    "    2022: \"https://datos.madrid.es/egob/catalogo/201200-10306318-calidad-aire-horario.zip\",\n",
    "    2021: \"https://datos.madrid.es/egob/catalogo/201200-10306317-calidad-aire-horario.zip\",\n",
    "    2020: \"https://datos.madrid.es/egob/catalogo/201200-10306316-calidad-aire-horario.zip\",\n",
    "    2019: \"https://datos.madrid.es/egob/catalogo/201200-42-calidad-aire-horario.zip\"\n",
    "}\n",
    "\n",
    "for anio, url in urls.items():\n",
    "    print(f\"Descargando {anio}...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "        for file_info in z.infolist():\n",
    "            if file_info.filename.lower().endswith(\".csv\"):\n",
    "               filename = os.path.basename(file_info.filename)\n",
    "               with z.open(file_info) as source, open(os.path.join(input_folder_aire, filename), 'wb') as target:\n",
    "                  target.write(source.read())\n",
    "    print(f\"{anio} descargado y extraído.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2B-5sMezF30"
   },
   "source": [
    "No he automatizado la descarga de los archivos de meteorología debido a que las urls de los csv no siguen un patrón constante y supondría poner manualmente las urls de cada mes, se pueden descargar aquí: https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=fa8357cec5efa610VgnVCM1000001d4a900aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default , descargar solo desde 2019 hasta 2024y añadirlos a la carpeta /content/sample_data/Meteorología_Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOSWFbe8Mr_Q"
   },
   "source": [
    "Lo primero que vamos a realizar es la conversión de las horas de columnas a filas para los datos de calidad del aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_ii5ta9MqB0",
    "outputId": "e5053df1-9914-4540-be22-1d38c3c886f5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "input_folder = \"/content/sample_data/Calidad_del_aire_original\"\n",
    "output_folder = \"/content/sample_data/Calidad_del_aire_1_transformacion\"\n",
    "\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "csv_files = glob(os.path.join(input_folder, \"*.csv\"))\n",
    "\n",
    "for file_path in csv_files:\n",
    "  print(f\"Procesando archivo: {os.path.basename(file_path)}\")\n",
    "  df = pd.read_csv(file_path, sep=\";\", encoding=\"latin1\")\n",
    "  rows = []\n",
    "  for _, row in df.iterrows():\n",
    "      base_data = row[['PROVINCIA', 'MUNICIPIO', 'ESTACION', 'MAGNITUD',\n",
    "                      'PUNTO_MUESTREO', 'ANO', 'MES', 'DIA']]\n",
    "      for hour in range(1, 25):\n",
    "          h_col = f'H{hour:02d}'\n",
    "          v_col = f'V{hour:02d}'\n",
    "          new_row = {\n",
    "              'PROVINCIA': base_data['PROVINCIA'],\n",
    "              'MUNICIPIO': base_data['MUNICIPIO'],\n",
    "              'ESTACION': base_data['ESTACION'],\n",
    "              'MAGNITUD': base_data['MAGNITUD'],\n",
    "              'PUNTO_MUESTREO': base_data['PUNTO_MUESTREO'],\n",
    "              'ANO': base_data['ANO'],\n",
    "              'MES': base_data['MES'],\n",
    "              'DIA': base_data['DIA'],\n",
    "              'HORA': hour,\n",
    "              'VALOR_HORA': row[h_col],\n",
    "              'VALIDACION': row[v_col]\n",
    "          }\n",
    "          rows.append(new_row)\n",
    "  df_transformado = pd.DataFrame(rows)\n",
    "  output_file = os.path.join(\n",
    "    output_folder, os.path.basename(file_path).replace(\".csv\", \"_transformado.csv\"))\n",
    "  df_transformado.to_csv(output_file, index=False)\n",
    "print(\"Transformación completada para todos los archivos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWHvYEWbMqzO"
   },
   "source": [
    "Haremos ahora lo mismo para los csv de meteorología"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nGYzFTu8TnGT",
    "outputId": "be897c8e-f466-4a60-f168-f8b3b56fb940"
   },
   "outputs": [],
   "source": [
    "input_folder='/content/sample_data/Meteorología_Original'\n",
    "output_folder='/content/sample_data/Meteorología_1_transformacion'\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "csv_files = glob(os.path.join(input_folder, \"*.csv\"))\n",
    "\n",
    "for file_path in csv_files:\n",
    "    print(f\"Procesando archivo: {os.path.basename(file_path)}\")\n",
    "\n",
    "    df = pd.read_csv(file_path, sep=\";\", encoding=\"latin1\")\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        base_data = row[['PROVINCIA', 'MUNICIPIO', 'ESTACION', 'MAGNITUD',\n",
    "                         'PUNTO_MUESTREO', 'ANO', 'MES', 'DIA']]\n",
    "        for hour in range(1, 25):\n",
    "            h_col = f'H{hour:02d}'\n",
    "            v_col = f'V{hour:02d}'\n",
    "            new_row = {\n",
    "                'PROVINCIA': base_data['PROVINCIA'],\n",
    "                'MUNICIPIO': base_data['MUNICIPIO'],\n",
    "                'ESTACION': base_data['ESTACION'],\n",
    "                'MAGNITUD': base_data['MAGNITUD'],\n",
    "                'PUNTO_MUESTREO': base_data['PUNTO_MUESTREO'],\n",
    "                'ANO': base_data['ANO'],\n",
    "                'MES': base_data['MES'],\n",
    "                'DIA': base_data['DIA'],\n",
    "                'HORA': hour,\n",
    "                'VALOR_HORA': row[h_col],\n",
    "                'VALIDACION': row[v_col]\n",
    "            }\n",
    "            rows.append(new_row)\n",
    "\n",
    "    df_transformado = pd.DataFrame(rows)\n",
    "\n",
    "    output_file = os.path.join(\n",
    "        output_folder, os.path.basename(file_path).replace(\".csv\", \"_transformado.csv\"))\n",
    "    df_transformado.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Transformación completada para todos los archivos de meteorología.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1ZXwuF2UkfV"
   },
   "source": [
    "Vale, visualizaremos los datos, para ver que tenemos las mismas columnas en los dos, cojeremos archivos aleatorios de cada carpeta ya que al haber sido transformados con el mismo método,si uno está bien, lo estarán todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PLQf6SlUsBN",
    "outputId": "18a49b49-a1d8-450b-f118-511aa088402f"
   },
   "outputs": [],
   "source": [
    "carpeta_aire = \"/content/sample_data/Calidad_del_aire_1_transformacion\"\n",
    "carpeta_meteo = \"/content/sample_data/Meteorología_1_transformacion\"\n",
    "import random\n",
    "archivo_aire = random.choice(glob(os.path.join(carpeta_aire, \"*.csv\")))\n",
    "archivo_meteo = random.choice(glob(os.path.join(carpeta_meteo, \"*.csv\")))\n",
    "\n",
    "df_aire = pd.read_csv(archivo_aire)\n",
    "df_meteo = pd.read_csv(archivo_meteo)\n",
    "\n",
    "print(f\"Archivo de calidad del aire cargado: {os.path.basename(archivo_aire)}\")\n",
    "print(df_aire.info())\n",
    "\n",
    "print(f\"Archivo de meteorología cargado: {os.path.basename(archivo_meteo)}\")\n",
    "print(df_meteo.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRZZEFQtWMZ5"
   },
   "source": [
    "Vale, ahora hemos visto que tienen 11 columnas los dos entonces está correcto, hemos visto también los tipos de dato de cada columna.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVi_6F_cWis5"
   },
   "source": [
    "Ahora vamos a plantear el ejercicio, una vez tenemos ya las estructuras principales organizadas, lo que vamos a querer hacer va a ser lo siguiente, queremos que la estructura final de nuestra base de datos, sea de las columnas ESTACION_calidad,MAGNITUD,ANO,MES,DIA,HORA,VALOR_HORA,VALOR_HORA_80, VALOR_HORA_81,VALOR_HORA_82,VALOR_HORA_83,VALOR_HORA_86,VALOR_HORA_87,VALOR_HORA_88,VALOR_HORA_89, siendo VALOR_HORA la variable objetivo, lo que haremos una vez tengamos esto es hacer one-hot encoding en las variables estacion y magnitud, así no asignará mas importancia a una magnitud por tener un número mayor, simplemente las organizará asignandole un 1 a cada una poniéndolas en columnas, con año, mes, dia y hora, las haremos variables cíclicas usando senos y cosenos, por último normalizaremos los datos para que vayan todos de 0 a 1 en función de su magnitud y columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMo21QkWXrzk"
   },
   "source": [
    "Siguiente paso pasar los csv de meteorología a PROVINCIA\tMUNICIPIO\tESTACION\tANO\tMES\tDIA\tHORA\tVALOR_HORA_80\tVALOR_HORA_81\tVALOR_HORA_82\tVALOR_HORA_83\tVALOR_HORA_86\tVALOR_HORA_87\tVALOR_HORA_88\tVALOR_HORA_89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0eiQP26Y1Fc",
    "outputId": "3ff8781c-abda-4a6d-c15a-3203c6acd08e"
   },
   "outputs": [],
   "source": [
    "input_folder = '/content/sample_data/Meteorología_1_transformacion'\n",
    "output_folder = '/content/sample_data/Meteorología_2_transformacion'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "csv_files = glob(os.path.join(input_folder, \"*.csv\"))\n",
    "for file_path in csv_files:\n",
    "    print(f\"Procesando {os.path.basename(file_path)}...\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df_pivot = df.pivot_table(\n",
    "        index=['PROVINCIA', 'MUNICIPIO', 'ESTACION', 'ANO', 'MES', 'DIA', 'HORA'],\n",
    "        columns='MAGNITUD',\n",
    "        values='VALOR_HORA'\n",
    "    ).reset_index()\n",
    "    df_pivot.columns.name = None\n",
    "    df_pivot = df_pivot.rename(columns={m: f'VALOR_HORA_{int(m)}' for m in df_pivot.columns if isinstance(m, (int, float))})\n",
    "\n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).replace(\".csv\", \"2_transformacion.csv\"))\n",
    "    df_pivot.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "print(\"Todos los archivos han sido transformados y guardados.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhtQbkASbh-V"
   },
   "source": [
    "Comprobemos que está todo en orden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "VQQp_HB-apzf",
    "outputId": "89051078-1472-4a5a-e35f-15c6bf8c38df"
   },
   "outputs": [],
   "source": [
    "archivo_random_nueva_tranformacion=random.choice(glob(os.path.join('/content/sample_data/Meteorología_2_transformacion', \"*.csv\")))\n",
    "df = pd.read_csv(archivo_random_nueva_tranformacion)\n",
    "print(f\"Examinando el siguiente archivo {os.path.basename(archivo_random_nueva_tranformacion)}...\")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j54cx1ilcUEr"
   },
   "source": [
    "Vemos que la magnitud 80 no la está cogiendo, vamos a ver porque, para ello, examinaremos todos los cvs de la carpeta antes de transformar y ver que archivos si que tenían en la columna magnitud, el valor 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbB5BG8wcaMK",
    "outputId": "b8efa5c3-bcfa-46a5-f198-3b0a4bed4e46"
   },
   "outputs": [],
   "source": [
    "folder_path = \"/content/sample_data/Meteorología_1_transformacion\"\n",
    "\n",
    "csv_files = glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "total_archivos = 0\n",
    "archivos_con_magnitud_80 = 0\n",
    "\n",
    "for file_path in csv_files:\n",
    "    total_archivos += 1\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'MAGNITUD' in df.columns and (df['MAGNITUD'] == 80).any():\n",
    "            archivos_con_magnitud_80 += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo {file_path}: {e}\")\n",
    "\n",
    "print(f\"Total de archivos analizados: {total_archivos}\")\n",
    "print(f\"Archivos que contienen MAGNITUD 80: {archivos_con_magnitud_80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uaeT-Mldahf"
   },
   "source": [
    "Vemos que solo hay 5 archivos que tienen magnitud 80, ahora sería interesante comprobar cuanta cantidasd de valores de magnitud 80 tenemso en esos 5 archivos comparados con la cantidad de informacion que tenemos de otras magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZdy50b_dnOS",
    "outputId": "8d4541d1-7d9f-4890-c755-0a21a71972d4"
   },
   "outputs": [],
   "source": [
    "conteo_total = {}\n",
    "\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'MAGNITUD' in df.columns:\n",
    "            for magnitud in df['MAGNITUD'].unique():\n",
    "                count = len(df[df['MAGNITUD'] == magnitud])\n",
    "                conteo_total[magnitud] = conteo_total.get(magnitud, 0) + count\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {file_path}: {e}\")\n",
    "\n",
    "print(\"Conteo de registros por MAGNITUD:\")\n",
    "for magnitud, count in sorted(conteo_total.items()):\n",
    "    print(f\"MAGNITUD {int(magnitud)}: {count} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOw76XTCsCj_"
   },
   "source": [
    "Vemos que comparado con la cantidad de valores nulos que nos va a dar la magnitud 80, no nos vale la pena tenerlo en cuenta, es una decisión que se ha tomado\n",
    "durante el procesamiento de los datos, así que vamos a cargarnos las columnas de valor_80 que tengamos en los archivos transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Hwuq532eZeB",
    "outputId": "0c75f3f5-115d-43c1-ed40-405c354f589d"
   },
   "outputs": [],
   "source": [
    "folder_path = \"/content/sample_data/Meteorología_2_transformacion\"\n",
    "csv_files = glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "archivos_con_valor_80 = 0\n",
    "\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if 'VALOR_HORA_80' in df.columns:\n",
    "            archivos_con_valor_80 += 1\n",
    "            df.drop(columns=['VALOR_HORA_80'], inplace=True)\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en {file_path}: {e}\")\n",
    "\n",
    "print(f\"Total de archivos analizados: {len(csv_files)}\")\n",
    "print(f\"Archivos que tenían la columna 'VALOR_HORA_80' y fueron modificados: {archivos_con_valor_80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVPYuYLOgLW5"
   },
   "source": [
    "Ahora contaremos cuantos csv tienen la columna VALOR_HORA_80 para asegurarnos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0pPCdTfgZ7Q",
    "outputId": "8b119e08-2952-42e6-b955-e090e89f04a5"
   },
   "outputs": [],
   "source": [
    "valor_80=0\n",
    "for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'VALOR_HORA_80' in df.columns:\n",
    "            archivos_con_valor_80 += 1\n",
    "print(archivos_con_valor_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hG9dzYmNh8Ic"
   },
   "source": [
    "Genial, ya está completado este proceso, como queremos combinar los df de meteorología y de calidad del aire, tendremos que ver que estaciones coinciden y cuales están cercas de otras\n",
    "para así poder asociar los datos de calidad del aire con los datos de meteorología. Automatizamos la descarga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUa_6ieaifzp",
    "outputId": "1fc7985b-9061-4a05-9f65-5b93a9d00534"
   },
   "outputs": [],
   "source": [
    "\n",
    "carpeta_estaciones='/content/sample_data/Estaciones'\n",
    "os.makedirs(carpeta_estaciones, exist_ok=True)\n",
    "\n",
    "archivos = {\n",
    "    \"https://datos.madrid.es/egob/catalogo/212629-1-estaciones-control-aire.csv\":\n",
    "        os.path.join(carpeta_estaciones, \"informacion_estaciones_red_calidad_aire.csv\"),\n",
    "\n",
    "    \"https://datos.madrid.es/egob/catalogo/300360-1-meteorologicos-estaciones.csv\":\n",
    "        os.path.join(carpeta_estaciones, \"Estaciones_control_datos_meteorologicos.csv\")\n",
    "}\n",
    "for url, destino in archivos.items():\n",
    "    print(f\"Descargando {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(destino, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Guardado en {destino}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar {url}: {e}\")\n",
    "file_estaciones_meteo = '/content/sample_data/Estaciones/Estaciones_control_datos_meteorologicos.csv'\n",
    "file_estaciones_aire = '/content/sample_data/Estaciones/informacion_estaciones_red_calidad_aire.csv'\n",
    "df_estaciones_meteo = pd.read_csv(file_estaciones_meteo, sep=\";\")\n",
    "df_estaciones_aire = pd.read_csv(file_estaciones_aire, sep=\";\")\n",
    "print(df_estaciones_meteo.columns.tolist())\n",
    "print(df_estaciones_aire.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBd5GtqYlZtr"
   },
   "source": [
    "Veamos cuantas estaciones tienen cada uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LeElyevFlZTT",
    "outputId": "29216b36-bdd5-4023-f0f4-a374c378cf5b"
   },
   "outputs": [],
   "source": [
    "num_estaciones_meteo = df_estaciones_meteo['ESTACION'].nunique()\n",
    "num_estaciones_aire = df_estaciones_aire['ESTACION'].nunique()\n",
    "\n",
    "print(f\"Estaciones meteorológicas únicas: {num_estaciones_meteo}\")\n",
    "print(f\"Estaciones de calidad del aire únicas: {num_estaciones_aire}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEGsTQpGlpPA"
   },
   "source": [
    "Vemos que hay mas estaciones de meteorología, pero entre las que no coincidan, nos vamos a quedar con las estaciones de calidad del aire, ya que es el df que contiene la variable objetivo, por lo tanto entre las que no coincidan, veremos cual es la estación de meteorología que está mas cerca y se la asociaremos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fABEYFEOkD-D"
   },
   "source": [
    "Ver las que coinciden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuMyaaK-kBBp",
    "outputId": "3dcea7ae-7c3d-438d-8024-6fc46f437912"
   },
   "outputs": [],
   "source": [
    "df_estaciones_meteo = df_estaciones_meteo.rename(columns={'CÓDIGO_CORTO': 'CODIGO_CORTO'})\n",
    "coincidentes = pd.merge(\n",
    "    df_estaciones_meteo,\n",
    "    df_estaciones_aire,\n",
    "    on='CODIGO_CORTO',\n",
    "    suffixes=('_meteo', '_aire')\n",
    ")\n",
    "print(\"Estaciones que coinciden por CODIGO_CORTO:\")\n",
    "print(coincidentes[['CODIGO_CORTO', 'ESTACION_meteo', 'ESTACION_aire']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CWzRN8-mHEC"
   },
   "source": [
    "Crearemos un mapa con folium (visto en clase) para poner en el mapa las estaciones meterológicas de un color y las de calidad del aire de otro color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vvCibs9i3oh",
    "outputId": "9005c73a-81eb-483b-fdf5-3639d14766de"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "mapa = folium.Map(location=[40.42, -3.70], zoom_start=11)\n",
    "\n",
    "for _, row in df_estaciones_meteo.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['LATITUD'], row['LONGITUD']],\n",
    "        radius=5,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Meteo: {row['ESTACION']}\"\n",
    "    ).add_to(mapa)\n",
    "\n",
    "for _, row in df_estaciones_aire.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['LATITUD'], row['LONGITUD']],\n",
    "        radius=5,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Aire: {row['ESTACION']}\"\n",
    "    ).add_to(mapa)\n",
    "\n",
    "\n",
    "os.makedirs('/content/MAPA', exist_ok=True)\n",
    "\n",
    "mapa.save(\"/content/MAPA/mapa.html\")\n",
    "print(\"Mapa generado: mapa.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnx56u-Sopoz"
   },
   "source": [
    "Veamos las que están cerca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1012
    },
    "id": "bOTYz4ARnHJr",
    "outputId": "3bc60b08-1bd9-4974-eef7-32dab2525b34"
   },
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "codigos_coincidentes = set(coincidentes['CODIGO_CORTO'])\n",
    "\n",
    "\n",
    "aire_sin_pareja = df_estaciones_aire[~df_estaciones_aire['CODIGO_CORTO'].isin(codigos_coincidentes)]\n",
    "\n",
    "\n",
    "candidatas_meteo = df_estaciones_meteo.copy()\n",
    "\n",
    "asociaciones = []\n",
    "\n",
    "for _, aire in aire_sin_pareja.iterrows():\n",
    "    punto_aire = (aire['LATITUD'], aire['LONGITUD'])\n",
    "    distancia_min = float('inf')\n",
    "    estacion_meteo_cercana = None\n",
    "    codigo_meteo_cercano = None\n",
    "\n",
    "    for _, meteo in candidatas_meteo.iterrows():\n",
    "        punto_meteo = (meteo['LATITUD'], meteo['LONGITUD'])\n",
    "        dist = geodesic(punto_aire, punto_meteo).meters\n",
    "        if dist < distancia_min:\n",
    "            distancia_min = dist\n",
    "            estacion_meteo_cercana = meteo['ESTACION']\n",
    "            codigo_meteo_cercano = meteo['CODIGO_CORTO']\n",
    "\n",
    "    asociaciones.append({\n",
    "        \"CODIGO_CORTO_AIRE\": aire['CODIGO_CORTO'],\n",
    "        \"ESTACION_aire\": aire['ESTACION'],\n",
    "        \"CODIGO_CORTO_METEO_MAS_CERCANA\": codigo_meteo_cercano,\n",
    "        \"ESTACION_meteo_mas_cercana\": estacion_meteo_cercana,\n",
    "        \"DISTANCIA_M\": round(distancia_min, 2)\n",
    "    })\n",
    "\n",
    "\n",
    "df_asociaciones = pd.DataFrame(asociaciones)\n",
    "df_asociaciones_sorted = df_asociaciones.sort_index()\n",
    "num=df_asociaciones_sorted.count()\n",
    "df_asociaciones_sorted.head(num[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKuQGWmmnG8d"
   },
   "source": [
    "Genial, ya tenemos las más cercanas, crearemos un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKESKKNrq21O",
    "outputId": "61756579-dc3f-4b85-a116-eaf3dca0c306"
   },
   "outputs": [],
   "source": [
    "equivalencias = {\n",
    "    str(row['CODIGO_CORTO']).zfill(3): str(row['CODIGO_CORTO']).zfill(3)\n",
    "    for _, row in coincidentes.iterrows()\n",
    "}\n",
    "\n",
    "for _, row in df_asociaciones.iterrows():\n",
    "    equivalencias[str(row['CODIGO_CORTO_AIRE']).zfill(3)] = str(row['CODIGO_CORTO_METEO_MAS_CERCANA']).zfill(3)\n",
    "\n",
    "print(equivalencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xe9AvBhyDGdR"
   },
   "source": [
    "Ahora borraremos las columnas que no aportan valor añadido de los csv, van a ser PROVINCIA, MUNICIPIO y PUNTO_MUESTREO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6-7ReYwDQUz",
    "outputId": "49d20480-99cf-4fb8-f327-35734639ddb3"
   },
   "outputs": [],
   "source": [
    "columnas_a_eliminar = ['PROVINCIA', 'MUNICIPIO', 'PUNTO_MUESTREO']\n",
    "\n",
    "carpetas = {\n",
    "    \"/content/sample_data/Calidad_del_aire_1_transformacion\": \"/content/sample_data/Calidad_del_aire_2_transformacion\"\n",
    "    \"/content/sample_data/Meteorología_2_transformacion\": \"/content/sample_data/Meteorología_3_transformacion\"\n",
    "}\n",
    "\n",
    "for carpeta_salida in carpetas.values():\n",
    "    os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "for carpeta_entrada, carpeta_salida in carpetas.items():\n",
    "    for nombre_archivo in os.listdir(carpeta_entrada):\n",
    "        if nombre_archivo.endswith(\".csv\"):\n",
    "            ruta_entrada = os.path.join(carpeta_entrada, nombre_archivo)\n",
    "            df = pd.read_csv(ruta_entrada)\n",
    "            df = df.drop(columns=columnas_a_eliminar, errors='ignore')\n",
    "            nuevo_nombre = nombre_archivo.replace(\".csv\", \"3.csv\")\n",
    "            ruta_salida = os.path.join(carpeta_salida, nuevo_nombre)\n",
    "            df.to_csv(ruta_salida, index=False)\n",
    "            print(f\"Guardado: {ruta_salida}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJk1pB6lE6W-"
   },
   "source": [
    "Comprobemos que esta todo en orden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "2ghXVJd_FHaq",
    "outputId": "ecb9429b-cccd-4dd1-99d8-c0e7f9aca623"
   },
   "outputs": [],
   "source": [
    "\n",
    "for nombre_carpeta, ruta in carpetas.items():\n",
    "    archivos = [f for f in os.listdir(ruta) if f.endswith(\".csv\")]\n",
    "    if archivos:\n",
    "        archivo_aleatorio = random.choice(archivos)\n",
    "        ruta_archivo = os.path.join(ruta, archivo_aleatorio)\n",
    "        print(f\"\\nCarpeta: {nombre_carpeta}\")\n",
    "        print(f\"Archivo seleccionado: {archivo_aleatorio}\")\n",
    "        df = pd.read_csv(ruta_archivo)\n",
    "        print(\"\\nColumnas y tipos de dato:\")\n",
    "        print(df.dtypes)\n",
    "    else:\n",
    "        print(f\"No se encontraron archivos en la carpeta: {nombre_carpeta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Evnb1o9QGEaV"
   },
   "source": [
    "Genial, vamos a ver ahora cuantos valores de VALOR_HORA de calidad del aire estan validados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99ImTT5AGKW7",
    "outputId": "8efdf0b5-be2f-4b35-a59f-97d39cbec624"
   },
   "outputs": [],
   "source": [
    "carpeta = \"/content/sample_data/Calidad_del_aire_2_transformacion\"\n",
    "\n",
    "total_v = 0\n",
    "total_n = 0\n",
    "for archivo in os.listdir(carpeta):\n",
    "    if archivo.endswith(\".csv\"):\n",
    "        ruta = os.path.join(carpeta, archivo)\n",
    "        df = pd.read_csv(ruta)\n",
    "        if 'VALIDACION' in df.columns:\n",
    "            total_v += (df['VALIDACION'] == 'V').sum()\n",
    "            total_n += (df['VALIDACION'] == 'N').sum()\n",
    "\n",
    "print(f\"Total 'V': {total_v}\")\n",
    "print(f\"Total 'N': {total_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RxSzPQlHLHD"
   },
   "source": [
    "Los valores sin validar constituyen menos del 2 por ciento de los datos, la decisión es eliminarlos para mejorar la calidad del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDsRBa53HVBS",
    "outputId": "722b9b91-4659-40ef-8bfb-d23100dba659"
   },
   "outputs": [],
   "source": [
    "carpeta_entrada = \"/content/sample_data/Calidad_del_aire_2_transformacion\"\n",
    "carpeta_salida = \"/content/sample_data/Calidad_del_aire_3_transformacion\"\n",
    "\n",
    "os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "for archivo in os.listdir(carpeta_entrada):\n",
    "    if archivo.endswith(\".csv\"):\n",
    "        ruta_entrada = os.path.join(carpeta_entrada, archivo)\n",
    "        try:\n",
    "            df = pd.read_csv(ruta_entrada)\n",
    "            if 'VALIDACION' in df.columns:\n",
    "                df = df[df['VALIDACION'] == 'V']\n",
    "            nuevo_nombre = archivo.replace(\".csv\", \"4.csv\")\n",
    "            ruta_salida = os.path.join(carpeta_salida, nuevo_nombre)\n",
    "            df.to_csv(ruta_salida, index=False)\n",
    "            print(f\"Guardado sin 'N': {ruta_salida}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {archivo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APt4OY3vHm_W"
   },
   "source": [
    "Comprobemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9oT0slHHlH2",
    "outputId": "c01a3136-57f5-4801-c2a6-6602a46fbf2d"
   },
   "outputs": [],
   "source": [
    "carpeta = \"/content/sample_data/Calidad_del_aire_3_transformacion\"\n",
    "\n",
    "total_v = 0\n",
    "total_n = 0\n",
    "for archivo in os.listdir(carpeta):\n",
    "    if archivo.endswith(\".csv\"):\n",
    "        ruta = os.path.join(carpeta, archivo)\n",
    "        df = pd.read_csv(ruta)\n",
    "        if 'VALIDACION' in df.columns:\n",
    "            total_v += (df['VALIDACION'] == 'V').sum()\n",
    "            total_n += (df['VALIDACION'] == 'N').sum()\n",
    "\n",
    "print(f\"Total 'V': {total_v}\")\n",
    "print(f\"Total 'N': {total_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QstbYfr2H5as"
   },
   "source": [
    "Ahora que ya tenemos solo V en la columna VALIDACION, la eliminamos ya que ya no nos aporta valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNKocMxMH42U",
    "outputId": "0dd96a11-2606-4d59-fe6a-a4d0b4d8b0a3"
   },
   "outputs": [],
   "source": [
    "carpeta_entrada = \"/content/sample_data/Calidad_del_aire_3_transformacion\"\n",
    "carpeta_salida = \"/content/sample_data/Calidad_del_aire_4_transformacion\"\n",
    "\n",
    "os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "for archivo in os.listdir(carpeta_entrada):\n",
    "    if archivo.endswith(\".csv\"):\n",
    "        ruta_entrada = os.path.join(carpeta_entrada, archivo)\n",
    "        try:\n",
    "            df = pd.read_csv(ruta_entrada)\n",
    "            df = df.drop(columns=['VALIDACION'], errors='ignore')\n",
    "            nuevo_nombre = archivo.replace(\".csv\", \"5.csv\")\n",
    "            ruta_salida = os.path.join(carpeta_salida, nuevo_nombre)\n",
    "            df.to_csv(ruta_salida, index=False)\n",
    "            print(f\"VALIDACION eliminada y guardado: {ruta_salida}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {archivo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lDovBWqIN8U"
   },
   "source": [
    "Comprobacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "ZGbel1eVIQTF",
    "outputId": "e4b34208-d132-44a0-8048-201c70b3d570"
   },
   "outputs": [],
   "source": [
    "carpeta = \"/content/sample_data/Calidad_del_aire_4_transformacion\"\n",
    "archivos = [f for f in os.listdir(carpeta) if f.endswith(\".csv\")]\n",
    "archivo_random = random.choice(archivos)\n",
    "ruta_archivo = os.path.join(carpeta, archivo_random)\n",
    "df = pd.read_csv(ruta_archivo)\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejKniEQYIbKT"
   },
   "source": [
    "Genial, los datasets ya están preparados para juntarlos de forma que quede de la siguiente manera 'ESTACION_CA'(ya que es la estacion de calidad del aire la que se va a a mantener en el dataset), 'MAGNITUD', 'ANO', 'MES', 'DIA', 'HORA', 'VALOR_HORA''VALOR_HORA_81''VALOR_HORA_82''VALOR_HORA_83''VALOR_HORA_86' 'VALOR_HORA_87''VALOR_HORA_88''VALOR_HORA_89'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oq67s8ux1mV5"
   },
   "source": [
    "Pero antes de juntarlos, voy a rellenar los valores nulos que hay en el dataset meteorológico, porque lo estaba intentando rellenar cuando ya estaba todo en el mismo dataframe, y es peor, ya que requiere de mucho más capacidad y de tiempo, entonces, lo que voy a hacer es los valores de magnitudes que sean nulos, sustituirlos por la media de la estación más cercana para esa magnitud, para ello haremos un dataframe de estaciones cercanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1DDFVNw2H-P",
    "outputId": "85cf4e6a-8d04-4ced-eae7-d42c4bb75bb1"
   },
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "import os\n",
    "\n",
    "df_aire = pd.read_csv(\"/content/sample_data/Estaciones/informacion_estaciones_red_calidad_aire.csv\", sep=\";\")\n",
    "df_meteo = pd.read_csv(\"/content/sample_data/Estaciones/Estaciones_control_datos_meteorologicos.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "df_aire['CODIGO_CORTO'] = df_aire['CODIGO_CORTO'].astype(str).str.zfill(3)\n",
    "df_meteo['CÓDIGO_CORTO'] = df_meteo['CÓDIGO_CORTO'].astype(str).str.zfill(3)\n",
    "\n",
    "\n",
    "estaciones_aire = df_aire[['CODIGO_CORTO', 'LATITUD', 'LONGITUD']].dropna()\n",
    "estaciones_meteo = df_meteo[['CÓDIGO_CORTO', 'LATITUD', 'LONGITUD']].dropna()\n",
    "\n",
    "distancias = []\n",
    "for _, est_a in estaciones_aire.iterrows():\n",
    "    for _, est_m in estaciones_meteo.iterrows():\n",
    "        punto_a = (est_a['LATITUD'], est_a['LONGITUD'])\n",
    "        punto_m = (est_m['LATITUD'], est_m['LONGITUD'])\n",
    "        dist = geodesic(punto_a, punto_m).kilometers\n",
    "        distancias.append({\n",
    "            'ESTACION_CA': est_a['CODIGO_CORTO'],\n",
    "            'ESTACION_METEO': est_m['CÓDIGO_CORTO'],\n",
    "            'DISTANCIA_KM': dist\n",
    "        })\n",
    "\n",
    "df_dist = pd.DataFrame(distancias)\n",
    "df_dist = df_dist.sort_values(['ESTACION_CA', 'DISTANCIA_KM'])\n",
    "\n",
    "output_path = \"/content/sample_data/Estaciones/equivalencias_estaciones_ordenadas.csv\"\n",
    "df_dist.to_csv(output_path, index=False)\n",
    "print(f\"Distancias guardadas en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euQfPON-2Oi7"
   },
   "source": [
    "Ahora realizaremos el relleno de los valores nulos de las columnas meteorológicas\n",
    "\n",
    "Hace lo siguiente:\n",
    "\n",
    "Para cada NaN, busca una estación cercana equivalente en la tabla de equivalencias.\n",
    "\n",
    "Intenta rellenar ese NaN con la media mensual de esa magnitud en la estación meteorológica cercana.\n",
    "\n",
    "Si encuentra una media válida (no NaN), la usa para rellenar el valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "Xy_CotHb2Kcz",
    "outputId": "759dec45-d7dd-4a4c-bfc2-90dbb5d79431"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "carpeta_origen = r\"/content/sample_data/Calidad_del_aire_4_transformacion\"\n",
    "carpeta_salida = r\"/content/sample_data/Calidad_del_aire_4_transformacion_Relleno\"\n",
    "ruta_equivalencias = r\"/content/sample_data/Estaciones/equivalencias_estaciones_ordenadas.csv\"\n",
    "\n",
    "\n",
    "os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "\n",
    "df_dist = pd.read_csv(ruta_equivalencias, dtype=str)\n",
    "df_dist['ESTACION_CA'] = df_dist['ESTACION_CA'].str.zfill(3)\n",
    "df_dist['ESTACION_METEO'] = df_dist['ESTACION_METEO'].str.zfill(3)\n",
    "\n",
    "\n",
    "archivos = [f for f in os.listdir(carpeta_origen) if f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "for archivo in tqdm(archivos, desc=\"Rellenando\"):\n",
    "    df = pd.read_csv(os.path.join(carpeta_origen, archivo))\n",
    "    df['ESTACION'] = df['ESTACION'].astype(str).str.zfill(3)\n",
    "\n",
    "    columnas_meteo = [col for col in df.columns if col.startswith(\"VALOR_HORA_8\")]\n",
    "\n",
    "    for col in columnas_meteo:\n",
    "        nulos = df[col].isna()\n",
    "        for idx in df[nulos].index:\n",
    "            row = df.loc[idx]\n",
    "            est_origen = row['ESTACION']\n",
    "            y, m = row['ANO'], row['MES']\n",
    "\n",
    "            estaciones_cercanas = df_dist[df_dist['ESTACION_CA'] == est_origen]['ESTACION_METEO'].tolist()\n",
    "            for est_cercana in estaciones_cercanas:\n",
    "                media = df[\n",
    "                    (df['ESTACION'] == est_cercana) &\n",
    "                    (df['ANO'] == y) &\n",
    "                    (df['MES'] == m)\n",
    "                ][col].mean(skipna=True)\n",
    "\n",
    "                if not np.isnan(media):\n",
    "                    df.at[idx, col] = media\n",
    "                    break\n",
    "\n",
    "\n",
    "    nuevo_nombre = archivo.replace(\".csv\", \"5.csv\")\n",
    "    ruta_salida = os.path.join(carpeta_salida, nuevo_nombre)\n",
    "    df.to_csv(ruta_salida, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "carpeta_origen = r\"/content/sample_data/Calidad_del_aire_4_transformacion_Relleno\"\n",
    "carpeta_salida = r\"/content/sample_data/Meteo_Transformado:Final\"\n",
    "os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "\n",
    "columnas_meteo = [f\"VALOR_HORA_8{i}\" for i in [1,2,3,6,7,8,9]]\n",
    "\n",
    "archivos = [f for f in os.listdir(carpeta_origen) if f.endswith(\".csv\")]\n",
    "\n",
    "for archivo in tqdm(archivos, desc=\"Rellenando con media mensual\"):\n",
    "    df = pd.read_csv(os.path.join(carpeta_origen, archivo))\n",
    "\n",
    "    for col in columnas_meteo:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "     \n",
    "        medias_mensuales = df.groupby(['ANO', 'MES'])[col].transform('mean')\n",
    "        df[col] = df[col].fillna(medias_mensuales)\n",
    "\n",
    "    nuevo_nombre = archivo.replace(\".csv\", \"_final.csv\")\n",
    "    df.to_csv(os.path.join(carpeta_salida, nuevo_nombre), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_ec3lZQ2mPw",
    "outputId": "f5bd7295-f3b0-4734-936d-c0c283a14aad"
   },
   "outputs": [],
   "source": [
    "ruta_drive = \"/content/drive/MyDrive/Mis_Datasets/MeteoFinal\"\n",
    "ruta_colab = \"/content/sample_data/Meteo_Transformado_Final\"\n",
    "\n",
    "shutil.copytree(ruta_drive, ruta_colab, dirs_exist_ok=True)\n",
    "print(\"Carpeta copiada a Colab.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2myjvKh4bH2"
   },
   "source": [
    "Vale, ahora ya tenemos los nuevos dato en la carpeta /content/sample_data/Meteo_Transformado_Final, ahora veamos la consistencia de los datos, contaremos los nulos y compararemos la media de las magnitudes cada mes antes de rellenar nulos y después (asi comprobaremos que los datos no han dado un cambio brusco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "2Kn2u3Kg4wuC",
    "outputId": "e1947828-05df-4e1c-d264-73dd2eba9d1f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "carpeta_original = \"/content/sample_data/Meteorología_3_transformacion\"\n",
    "carpeta_relleno = \"/content/sample_data/Meteo_Transformado_Final\"\n",
    "\n",
    "\n",
    "nulos_totales = defaultdict(int)\n",
    "medias_antes = defaultdict(list)\n",
    "medias_despues = defaultdict(list)\n",
    "\n",
    "\n",
    "for archivo in os.listdir(carpeta_original):\n",
    "    if archivo.endswith(\".csv\"):\n",
    "        archivo_relleno = archivo.replace(\".csv\", \"5_final.csv\")\n",
    "        ruta_ori = os.path.join(carpeta_original, archivo)\n",
    "        ruta_new = os.path.join(carpeta_relleno, archivo_relleno)\n",
    "\n",
    "        if not os.path.exists(ruta_new):\n",
    "            print(f\"Archivo faltante: {archivo_relleno}\")\n",
    "            continue\n",
    "\n",
    "        df_ori = pd.read_csv(ruta_ori)\n",
    "        df_new = pd.read_csv(ruta_new)\n",
    "\n",
    "        columnas_meteo = [col for col in df_new.columns if col.startswith(\"VALOR_HORA_8\")]\n",
    "\n",
    "        for col in columnas_meteo:\n",
    "            nulos_totales[col] += df_new[col].isna().sum()\n",
    "            medias_antes[col].append(df_ori[col].mean(skipna=True))\n",
    "            medias_despues[col].append(df_new[col].mean(skipna=True))\n",
    "\n",
    "\n",
    "print(\"Nulos por columna (en archivos con relleno):\")\n",
    "for col in sorted(nulos_totales):\n",
    "    print(f\"{col}: {nulos_totales[col]}\")\n",
    "\n",
    "print(\"\\nMedia total antes vs después de la imputación:\")\n",
    "for col in sorted(medias_antes):\n",
    "    media_a = pd.Series(medias_antes[col]).mean()\n",
    "    media_d = pd.Series(medias_despues[col]).mean()\n",
    "    print(f\"{col}: antes = {media_a:.2f} | después = {media_d:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzAA8hYZ7lSe"
   },
   "source": [
    "Vemos que ya no hay ningún nulo y en aspectos generales, mantiene una buena consistencia de la media a pesar de haber rellenado muchos valores (esto se debe a que el método de elegir la estación mas cercana tenía consistencia y sentido según el problema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5KyhJbHAouY"
   },
   "source": [
    "Ahora, vamos a combinar los datos, de forma que por cada valor_hora que será la variable objetivo en nuestro modelo, tenga variables de magnitudes meteorológicas, que han sido rellenadas previamente las que eran nulas con un lógica y de esta forma cada valor_hora tendra datos de todas las magnitudes meteorológicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Og6mcclJLuG",
    "outputId": "7c3664dc-f6d5-45d7-ebef-7888b2ec02d4"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "carpeta_calidad = \"/content/sample_data/Calidad_del_aire_4_transformacion\"\n",
    "carpeta_meteo = \"/content/sample_data/Meteo_Transformado_Final\"\n",
    "\n",
    "df_meteo_total = pd.concat(\n",
    "    [pd.read_csv(os.path.join(carpeta_meteo, f)) for f in os.listdir(carpeta_meteo) if f.endswith(\".csv\")],\n",
    "    ignore_index=True\n",
    ")\n",
    "df_meteo_total['ESTACION'] = df_meteo_total['ESTACION'].astype(str).str.zfill(3)\n",
    "\n",
    "\n",
    "datasets_combinados = []\n",
    "contador_uniones = 0\n",
    "\n",
    "\n",
    "archivos_ca = [f for f in os.listdir(carpeta_calidad) if f.endswith(\".csv\")]\n",
    "\n",
    "for archivo_ca in tqdm(archivos_ca, desc=\"Combinando datos\"):\n",
    "    df_ca = pd.read_csv(os.path.join(carpeta_calidad, archivo_ca))\n",
    "    df_ca['ESTACION'] = df_ca['ESTACION'].astype(str).str.zfill(3)\n",
    "\n",
    "    estaciones_ca = df_ca['ESTACION'].unique()\n",
    "\n",
    "    for estacion_ca in estaciones_ca:\n",
    "        if estacion_ca in equivalencias:\n",
    "            estacion_meteo = equivalencias[estacion_ca]\n",
    "\n",
    "            df_ca_filtrada = df_ca[df_ca['ESTACION'] == estacion_ca]\n",
    "            df_meteo_filtrada = df_meteo_total[df_meteo_total['ESTACION'] == estacion_meteo]\n",
    "\n",
    "            df_merged = pd.merge(\n",
    "                df_ca_filtrada,\n",
    "                df_meteo_filtrada,\n",
    "                on=['ANO', 'MES', 'DIA', 'HORA'],\n",
    "                suffixes=('', '_meteo')\n",
    "            )\n",
    "\n",
    "            if not df_merged.empty:\n",
    "                contador_uniones += 1\n",
    "                df_merged = df_merged.rename(columns={'ESTACION': 'ESTACION_CA'})\n",
    "                columnas_finales = ['ESTACION_CA', 'MAGNITUD', 'ANO', 'MES', 'DIA', 'HORA', 'VALOR_HORA']\n",
    "                columnas_meteo = [col for col in df_merged.columns if 'VALOR_HORA_' in col]\n",
    "                df_final = df_merged[columnas_finales + columnas_meteo]\n",
    "                datasets_combinados.append(df_final)\n",
    "\n",
    "df_dataset_completo = pd.concat(datasets_combinados, ignore_index=True)\n",
    "\n",
    "\n",
    "os.makedirs(\"/content/DatasetCompleto\", exist_ok=True)\n",
    "df_dataset_completo.to_csv(\"/content/DatasetCompleto/dataset_combinado_final.csv\", index=False)\n",
    "\n",
    "print(f\"Combinación completa. Total combinaciones exitosas: {contador_uniones}\")\n",
    "print(f\"Dataset final guardado con {len(df_dataset_completo)} registros.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zk2xBR4uOUty"
   },
   "source": [
    "Ahora vamos a ver cuantos datos VALOR_HORA ahi en el dataset fonal para ver que seguimos manteniendo el mismo numero de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIHrDzBSOa4R",
    "outputId": "56a26bb0-579a-4913-c0a6-cd4ce802fb3e"
   },
   "outputs": [],
   "source": [
    "total_valores_hora = df_dataset_completo['VALOR_HORA'].notna().sum()\n",
    "print(f\"Total de valores de VALOR_HORA en el dataset final: {total_valores_hora}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-jqMxqYOcgY"
   },
   "source": [
    "Ahora vamos a ver cuantos valores nulos hay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssUEbG9dOhBg",
    "outputId": "6f8edd02-a8de-4c84-e1de-5388c482c17a"
   },
   "outputs": [],
   "source": [
    "total_nulos = df_dataset_completo.isna().sum().sum()\n",
    "print(f\"Total de valores nulos en el dataset final: {total_nulos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjhvQKwSUXQ5"
   },
   "source": [
    "No hay nulos, el dataset está listo para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUr2ue8jUOqW",
    "outputId": "9b59a80a-1b6e-4a96-d828-e9d029773785"
   },
   "outputs": [],
   "source": [
    "nulos = df_dataset_completo.isna().sum()\n",
    "totales = df_dataset_completo.shape[0]\n",
    "\n",
    "resumen_nulos = pd.DataFrame({\n",
    "    'Nulos': nulos,\n",
    "    'Total': totales,\n",
    "    'Porcentaje_nulos (%)': (nulos / totales * 100).round(2)\n",
    "})\n",
    "\n",
    "resumen_nulos = resumen_nulos.sort_values(by='Nulos', ascending=False)\n",
    "\n",
    "print(\" Nulos, total y porcentaje por columna:\")\n",
    "print(resumen_nulos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwuX2_L7CTb0"
   },
   "source": [
    "Veamos más información del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 822
    },
    "id": "pNIvyIs3CTIk",
    "outputId": "8847b358-e405-4a80-dc8f-212d21849363"
   },
   "outputs": [],
   "source": [
    "df_dataset_completo.info()\n",
    "df_dataset_completo.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxPFUdAOgeRm"
   },
   "source": [
    "Ahora, realizaremos one-hot-encoding para las variables categóricas (ESTACION_CA Y MAGNITUD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qEcm59Cfk1Y",
    "outputId": "1f898a56-265b-4030-d44a-9d03494bb699"
   },
   "outputs": [],
   "source": [
    "ruta_entrada = \"/content/DatasetCompleto/dataset_combinado_final.csv\"\n",
    "df = pd.read_csv(ruta_entrada)\n",
    "\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=[\"ESTACION_CA\", \"MAGNITUD\"], prefix=[\"ESTACION_CA\", \"MAGNITUD\"])\n",
    "\n",
    "carpeta_salida = \"/content/DATASETFINAL_1_TRANSFORMACION\"\n",
    "os.makedirs(carpeta_salida, exist_ok=True)\n",
    "ruta_salida = os.path.join(carpeta_salida, \"dataset_final_encoded.csv\")\n",
    "df_encoded.to_csv(ruta_salida, index=False)\n",
    "\n",
    "print(f\"Dataset con one-hot encoding guardado en: {ruta_salida}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTDa3YWdDucR"
   },
   "source": [
    "Vamos comprobando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVOdBD3qCry6",
    "outputId": "f1be3183-423c-45d1-aabc-aab258b28fde"
   },
   "outputs": [],
   "source": [
    "df_encoded = pd.read_csv('/content/DATASETFINAL_1_TRANSFORMACION/dataset_final_encoded.csv')\n",
    "\n",
    "total_valores_post_encoding = df_encoded['VALOR_HORA'].notna().sum()\n",
    "\n",
    "print(f\"Total de valores de VALOR_HORA después del one-hot encoding: {total_valores_post_encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3FewLXTEB2m",
    "outputId": "db46086a-e2b4-4672-8f14-a5fad082485f"
   },
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xo0KLtzGELuF",
    "outputId": "ee49474d-603b-40ac-f492-07aaa661b799"
   },
   "outputs": [],
   "source": [
    "nulos = df_encoded.isna().sum()\n",
    "totales = df_encoded.shape[0]\n",
    "\n",
    "resumen_nulos = pd.DataFrame({\n",
    "    'Nulos': nulos,\n",
    "    'Total': totales,\n",
    "    'Porcentaje_nulos (%)': (nulos / totales * 100).round(2)\n",
    "})\n",
    "\n",
    "resumen_nulos = resumen_nulos.sort_values(by='Nulos', ascending=False)\n",
    "\n",
    "print(\" Nulos, total y porcentaje por columna:\")\n",
    "print(resumen_nulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ktc9llcSgvrI"
   },
   "source": [
    "Ahora transformaremos las variables temporales en variables cíclicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gog3E7Z5guyQ",
    "outputId": "8f5f3d2b-3bf1-489e-fd1f-94da2ba814f2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ruta_entrada = \"/content/DATASETFINAL_1_TRANSFORMACION/dataset_final_encoded.csv\"\n",
    "df = pd.read_csv(ruta_entrada)\n",
    "\n",
    "df['HORA_sin'] = np.sin(2 * np.pi * df['HORA'] / 24)\n",
    "df['HORA_cos'] = np.cos(2 * np.pi * df['HORA'] / 24)\n",
    "\n",
    "df['DIA_sin'] = np.sin(2 * np.pi * df['DIA'] / 31)\n",
    "df['DIA_cos'] = np.cos(2 * np.pi * df['DIA'] / 31)\n",
    "\n",
    "df['MES_sin'] = np.sin(2 * np.pi * df['MES'] / 12)\n",
    "df['MES_cos'] = np.cos(2 * np.pi * df['MES'] / 12)\n",
    "\n",
    "df['AÑO_sin'] = np.sin(2 * np.pi * (df['ANO'] - 2019) / 6)\n",
    "df['AÑO_cos'] = np.cos(2 * np.pi * (df['ANO'] - 2019) / 6)\n",
    "\n",
    "df = df.drop(columns=['HORA', 'DIA', 'MES', 'ANO'])\n",
    "\n",
    "carpeta_salida = \"/content/DATASETFINAL_2_TRANSFORMACION\"\n",
    "os.makedirs(carpeta_salida, exist_ok=True)\n",
    "ruta_salida = os.path.join(carpeta_salida, \"dataset_final_ciclico.csv\")\n",
    "df.to_csv(ruta_salida, index=False)\n",
    "\n",
    "print(f\"Dataset con variables cíclicas guardado en: {ruta_salida}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSs2kcoRE3hK"
   },
   "source": [
    "Comprobamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AivR73_NE5dw",
    "outputId": "b54ad280-7ec5-4feb-971e-6e12e338837d"
   },
   "outputs": [],
   "source": [
    "df_ciclico = pd.read_csv('/content/DATASETFINAL_2_TRANSFORMACION/dataset_final_ciclico.csv')\n",
    "total_valores = df_ciclico['VALOR_HORA'].notna().sum()\n",
    "\n",
    "print(f\"Total de valores de VALOR_HORA : {total_valores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiuOV-uBFjqg",
    "outputId": "da10e17b-aada-46d6-c685-46f8fc48093c"
   },
   "outputs": [],
   "source": [
    "df_ciclico.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUWXIr0DFc1G",
    "outputId": "5022275f-86de-4082-80cd-d56859738176"
   },
   "outputs": [],
   "source": [
    "nulos = df_ciclico.isna().sum()\n",
    "totales = df_ciclico.shape[0]\n",
    "\n",
    "resumen_nulos = pd.DataFrame({\n",
    "    'Nulos': nulos,\n",
    "    'Total': totales,\n",
    "    'Porcentaje_nulos (%)': (nulos / totales * 100).round(2)\n",
    "})\n",
    "\n",
    "resumen_nulos = resumen_nulos.sort_values(by='Nulos', ascending=False)\n",
    "\n",
    "print(\" Nulos, total y porcentaje por columna:\")\n",
    "print(resumen_nulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1Cs1wF2hyi4"
   },
   "source": [
    "Genial, ahora haremos la normalización de datos numéricos , el objetivo es escalar todas las variables numéricas a un rango [0.1] de forma que dejemos ya todas las variables listas para entrenar modelos de Machine Learning, se normalizan :\n",
    "\n",
    "VALOR_HORA\n",
    "VALOR_HORA_81\n",
    "VALOR_HORA_82\n",
    "VALOR_HORA_83\n",
    "VALOR_HORA_86\n",
    "VALOR_HORA_87\n",
    "VALOR_HORA_88\n",
    "VALOR_HORA_89\n",
    "HORA_sin, HORA_cos\n",
    "DIA_sin, DIA_cos\n",
    "MES_sin, MES_cos\n",
    "AÑO_sin, AÑO_cos\n",
    "\n",
    "Guardaremos el scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyJcyek1hv9g",
    "outputId": "05711263-6793-4dc1-92f3-3b0a52a6a9b2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "ruta_entrada = \"/content/DATASETFINAL_2_TRANSFORMACION/dataset_final_ciclico.csv\"\n",
    "df = pd.read_csv(ruta_entrada)\n",
    "\n",
    "\n",
    "columnas_a_normalizar = [\n",
    "    col for col in df.columns\n",
    "    if col.startswith(\"VALOR_HORA\") or col.endswith(\"_sin\") or col.endswith(\"_cos\")\n",
    "]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[columnas_a_normalizar] = scaler.fit_transform(df[columnas_a_normalizar])\n",
    "\n",
    "carpeta_salida = \"/content/DATASETFINAL_3_TRANSFORMACION\"\n",
    "os.makedirs(carpeta_salida, exist_ok=True)\n",
    "ruta_salida = os.path.join(carpeta_salida, \"dataset_final_normalizado.csv\")\n",
    "df.to_csv(ruta_salida, index=False)\n",
    "\n",
    "\n",
    "joblib.dump(scaler, '/content/scaler_magnitudes.pkl')\n",
    "print(\"Scaler guardado como 'scaler_magnitudes.pkl'\")\n",
    "\n",
    "\n",
    "print(f\"Dataset normalizado guardado en: {ruta_salida}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LLNrSU_j1rl"
   },
   "source": [
    "Veamos la información del dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "1YtkM_E_j8yu",
    "outputId": "dfb387e7-8be7-45c9-debe-916537e4f3be"
   },
   "outputs": [],
   "source": [
    "df_normalizado = pd.read_csv(\"/content/DATASETFINAL_3_TRANSFORMACION/dataset_final_normalizado.csv\")\n",
    "print(f\"Número total de filas: {len(df_normalizado)}\")\n",
    "columnas_a_comprobar = [\n",
    "    col for col in df_normalizado.columns\n",
    "    if col.startswith(\"VALOR_HORA\") or col.endswith(\"_sin\") or col.endswith(\"_cos\")\n",
    "]\n",
    "\n",
    "for col in columnas_a_comprobar:\n",
    "    min_val = df_normalizado[col].min()\n",
    "    max_val = df_normalizado[col].max()\n",
    "    if min_val < 0 or max_val > 1:\n",
    "        print(f\"{col} fuera del rango: min={min_val}, max={max_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qduz4MJGIYJk"
   },
   "source": [
    "Vemos que en la normalización hay un valor de la magnitud 83 que se ha salido del rango pero es tan tan tan pequeño, que esto no marcará ningun tipo de diferencia, lo más probable es que se deba simplemente a una imprecisión por coma flotante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WSP-QWxG4E2",
    "outputId": "1e5ce642-9896-454e-e4dc-f25afad267ba"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_normalizado = pd.read_csv('/content/DATASETFINAL_3_TRANSFORMACION/dataset_final_normalizado.csv')\n",
    "total_valores = df_normalizado['VALOR_HORA'].notna().sum()\n",
    "\n",
    "print(f\"Total de valores de VALOR_HORA : {total_valores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrWH34qkG0oJ",
    "outputId": "a1f2585f-a8ee-432a-e2c6-18d6fc40c80d"
   },
   "outputs": [],
   "source": [
    "nulos = df_normalizado.isna().sum()\n",
    "totales = df_normalizado.shape[0]\n",
    "\n",
    "resumen_nulos = pd.DataFrame({\n",
    "    'Nulos': nulos,\n",
    "    'Total': totales,\n",
    "    'Porcentaje_nulos (%)': (nulos / totales * 100).round(2)\n",
    "})\n",
    "\n",
    "resumen_nulos = resumen_nulos.sort_values(by='Nulos', ascending=False)\n",
    "\n",
    "print(\" Nulos, total y porcentaje por columna:\")\n",
    "print(resumen_nulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkVVutB3iVFm"
   },
   "source": [
    "A continuación entrenaremos el modelo en local, para ello he utilizado pytorch, me instalé pytorch y tambien me instalé cuda, la versión compatible con la versión que me había instalado de pytorch, esto me permitió utilizar mi gráfica de NVIDIA para entrenar el modelo lo cual me permitiría muchos mejores resultados,en mi caso, CUDA 11.8 era compatible con mi GPU y versión de drivers.Para aislar dependencias, utilicé un entorno virtual y en este entorno virtual ejecute mi entrenamiento:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
